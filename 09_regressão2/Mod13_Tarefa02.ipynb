{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EBAC - Regressão II - regressão múltipla\n",
    "\n",
    "## Tarefa I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Previsão de renda II\n",
    "\n",
    "Vamos continuar trabalhando com a base 'previsao_de_renda.csv', que é a base do seu próximo projeto. Vamos usar os recursos que vimos até aqui nesta base.\n",
    "\n",
    "|variavel|descrição|\n",
    "|-|-|\n",
    "|data_ref                | Data de referência de coleta das variáveis |\n",
    "|index                   | Código de identificação do cliente|\n",
    "|sexo                    | Sexo do cliente|\n",
    "|posse_de_veiculo        | Indica se o cliente possui veículo|\n",
    "|posse_de_imovel         | Indica se o cliente possui imóvel|\n",
    "|qtd_filhos              | Quantidade de filhos do cliente|\n",
    "|tipo_renda              | Tipo de renda do cliente|\n",
    "|educacao                | Grau de instrução do cliente|\n",
    "|estado_civil            | Estado civil do cliente|\n",
    "|tipo_residencia         | Tipo de residência do cliente (própria, alugada etc)|\n",
    "|idade                   | Idade do cliente|\n",
    "|tempo_emprego           | Tempo no emprego atual|\n",
    "|qt_pessoas_residencia   | Quantidade de pessoas que moram na residência|\n",
    "|renda                   | Renda em reais|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "import patsy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('previsao_de_renda.csv')\n",
    "\n",
    "df = df.drop(columns=['data_ref', 'Unnamed: 0', 'id_cliente'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15000 entries, 0 to 14999\n",
      "Data columns (total 12 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   sexo                   15000 non-null  object \n",
      " 1   posse_de_veiculo       15000 non-null  bool   \n",
      " 2   posse_de_imovel        15000 non-null  bool   \n",
      " 3   qtd_filhos             15000 non-null  int64  \n",
      " 4   tipo_renda             15000 non-null  object \n",
      " 5   educacao               15000 non-null  object \n",
      " 6   estado_civil           15000 non-null  object \n",
      " 7   tipo_residencia        15000 non-null  object \n",
      " 8   idade                  15000 non-null  int64  \n",
      " 9   tempo_emprego          12427 non-null  float64\n",
      " 10  qt_pessoas_residencia  15000 non-null  float64\n",
      " 11  renda                  15000 non-null  float64\n",
      "dtypes: bool(2), float64(3), int64(2), object(5)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Separe a base em treinamento e teste (25% para teste, 75% para treinamento).\n",
    "2. Rode uma regularização *ridge* com alpha = [0, 0.001, 0.005, 0.01, 0.05, 0.1] e avalie o $R^2$ na base de testes. Qual o melhor modelo?\n",
    "3. Faça o mesmo que no passo 2, com uma regressão *LASSO*. Qual método chega a um melhor resultado?\n",
    "4. Rode um modelo *stepwise*. Avalie o $R^2$ na vase de testes. Qual o melhor resultado?\n",
    "5. Compare os parâmetros e avalie eventuais diferenças. Qual modelo você acha o melhor de todos?\n",
    "6. Partindo dos modelos que você ajustou, tente melhorar o $R^2$ na base de testes. Use a criatividade, veja se consegue inserir alguma transformação ou combinação de variáveis.\n",
    "7. Ajuste uma árvore de regressão e veja se consegue um $R^2$ melhor com ela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1)\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "df_dummies = pd.get_dummies(df, drop_first=True)\n",
    "\n",
    "X = df_dummies.drop(columns=[\"renda\"])\n",
    "y = df_dummies[\"renda\"]\n",
    "\n",
    "# Separar dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "#X_train_const = sm.add_constant(X_train)\n",
    "#X_test_const = sm.add_constant(X_test)\n",
    "\n",
    "bool_columns = X_train.select_dtypes(include=[\"bool\"]).columns\n",
    "bool_columns2 = X_test.select_dtypes(include=[\"bool\"]).columns\n",
    "\n",
    "# Converter as colunas booleanas para inteiros\n",
    "X_train[bool_columns] = X_train[bool_columns].astype(int)\n",
    "X_test[bool_columns2] = X_test[bool_columns2].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados para cada alpha:\n",
      "Alpha: 0.000, R²: 0.2980\n",
      "Alpha: 0.001, R²: 0.2980\n",
      "Alpha: 0.005, R²: 0.2980\n",
      "Alpha: 0.010, R²: 0.2980\n",
      "Alpha: 0.050, R²: 0.2980\n",
      "Alpha: 0.100, R²: 0.2980\n",
      "\n",
      "Melhor modelo:\n",
      "Alpha: 0.100, R²: 0.2980\n"
     ]
    }
   ],
   "source": [
    "# 2)\n",
    "\n",
    "alphas = [0, 0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "\n",
    "# Lista para armazenar resultados\n",
    "results = []\n",
    "\n",
    "# Loop para testar cada valor de alpha\n",
    "for alpha in alphas:\n",
    "    # Criação e treinamento do modelo Ridge\n",
    "    ridge_model = Ridge(alpha=alpha)\n",
    "    ridge_model.fit(X_train_const, y_train)\n",
    "    \n",
    "    # Predição na base de teste\n",
    "    y_pred = ridge_model.predict(X_test_const)\n",
    "    \n",
    "    # Cálculo do R²\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Armazena o resultado\n",
    "    results.append((alpha, r2))\n",
    "\n",
    "# Identifica o melhor modelo (maior R²)\n",
    "best_alpha, best_r2 = max(results, key=lambda x: x[1])\n",
    "\n",
    "# Exibição dos resultados\n",
    "print(\"Resultados para cada alpha:\")\n",
    "for alpha, r2 in results:\n",
    "    print(f\"Alpha: {alpha:.3f}, R²: {r2:.4f}\")\n",
    "\n",
    "print(\"\\nMelhor modelo:\")\n",
    "print(f\"Alpha: {best_alpha:.3f}, R²: {best_r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\valdi\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  return fit_method(estimator, *args, **kwargs)\n",
      "C:\\Users\\valdi\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "C:\\Users\\valdi\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:628: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.036e+11, tolerance: 8.060e+07 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados para cada alpha (LASSO):\n",
      "Alpha: 0.000, R²: 0.2980\n",
      "Alpha: 0.001, R²: 0.2980\n",
      "Alpha: 0.005, R²: 0.2980\n",
      "Alpha: 0.010, R²: 0.2980\n",
      "Alpha: 0.050, R²: 0.2980\n",
      "Alpha: 0.100, R²: 0.2980\n",
      "\n",
      "Melhor modelo LASSO:\n",
      "Alpha: 0.100, R²: 0.2980\n"
     ]
    }
   ],
   "source": [
    "# 3)\n",
    "\n",
    "# Lista de valores de alpha\n",
    "alphas = [0, 0.001, 0.005, 0.01, 0.05, 0.1]\n",
    "\n",
    "# Lista para armazenar os resultados\n",
    "results_lasso = []\n",
    "\n",
    "# Loop para LASSO\n",
    "for alpha in alphas:\n",
    "    lasso_model = Lasso(alpha=alpha, max_iter=10000)  # max_iter aumentado para garantir convergência\n",
    "    lasso_model.fit(X_train, y_train)\n",
    "    y_pred_lasso = lasso_model.predict(X_test)\n",
    "    r2_lasso = r2_score(y_test, y_pred_lasso)\n",
    "    results_lasso.append((alpha, r2_lasso))\n",
    "\n",
    "# Melhor modelo LASSO\n",
    "best_alpha_lasso, best_r2_lasso = max(results_lasso, key=lambda x: x[1])\n",
    "\n",
    "# Exibição dos resultados\n",
    "print(\"Resultados para cada alpha (LASSO):\")\n",
    "for alpha, r2 in results_lasso:\n",
    "    print(f\"Alpha: {alpha:.3f}, R²: {r2:.4f}\")\n",
    "\n",
    "print(\"\\nMelhor modelo LASSO:\")\n",
    "print(f\"Alpha: {best_alpha_lasso:.3f}, R²: {best_r2_lasso:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possíveis razões para resultados idênticos:\n",
    "\n",
    "- Alpha baixo: Os valores de alpha testados (0,0.001,0.005,0.01,0.05,0.10,0.001,0.005,0.01,0.05,0.1) podem ser pequenos demais para que o LASSO tenha impacto, especialmente se os coeficientes ainda não foram reduzidos significativamente.\n",
    "- Escala dos dados: Se os dados não estão padronizados (média 0 e desvio padrão 1), o efeito do alpha pode ser mascarado. Ridge e LASSO são sensíveis à escala das variáveis.\n",
    "- Natureza dos dados: Se os dados têm poucas variáveis irrelevantes, o LASSO pode não estar zerando coeficientes, tornando os resultados similares aos do Ridge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add  tempo_emprego                  with p-value 0.000000\n",
      "Add  sexo_M                         with p-value 0.000000\n",
      "Add  tipo_renda_Empresário          with p-value 0.000004\n",
      "Add  idade                          with p-value 0.000024\n",
      "Add  educacao_Superior completo     with p-value 0.000804\n",
      "Add  posse_de_imovel                with p-value 0.043736\n",
      "resulting features:\n",
      "['tempo_emprego', 'sexo_M', 'tipo_renda_Empresário', 'idade', 'educacao_Superior completo', 'posse_de_imovel']\n"
     ]
    }
   ],
   "source": [
    "# 4)\n",
    "\n",
    "def stepwise_selection(X_train, y_train, \n",
    "                       initial_list=[], \n",
    "                       threshold_in=0.05, \n",
    "                       threshold_out = 0.05, \n",
    "                       verbose=True):\n",
    "    \"\"\" Perform a forward-backward feature selection \n",
    "    based on p-value from statsmodels.api.OLS \"\"\"\n",
    "    \n",
    "    included = list(initial_list)\n",
    "    \n",
    "    while True:\n",
    "        changed=False\n",
    "        \n",
    "        # Forward step: Seleciona as melhores variáveis para incluir\n",
    "        excluded = list(set(X.columns) - set(included))\n",
    "        new_pval = pd.Series(index=excluded, dtype=np.dtype('float64'))\n",
    "        \n",
    "        for new_column in excluded:\n",
    "            model = sm.OLS(y_train, sm.add_constant(X_train[included + [new_column]])).fit()\n",
    "            new_pval[new_column] = model.pvalues[new_column]\n",
    "        \n",
    "        best_pval = new_pval.min()\n",
    "        \n",
    "        # Se a variável a ser incluída tem p-value menor que o limiar de inclusão\n",
    "        if best_pval < threshold_in:\n",
    "            best_feature = new_pval.index[new_pval.argmin()]\n",
    "            included.append(best_feature)\n",
    "            changed = True\n",
    "            if verbose:\n",
    "                print(f'Add  {best_feature:30} with p-value {best_pval:.6f}')\n",
    "\n",
    "        # Backward step: Exclui as variáveis com maior p-value (acima de threshold_out)\n",
    "        if len(included) > 0:\n",
    "            model = sm.OLS(y_train, sm.add_constant(X_train[included])).fit()\n",
    "            pvalues = model.pvalues.iloc[1:]  # Ignora o intercepto\n",
    "            worst_pval = pvalues.max()  # Variável com maior p-value\n",
    "            \n",
    "            if worst_pval > threshold_out:\n",
    "                changed = True\n",
    "                worst_feature = pvalues.argmax()\n",
    "                included.remove(worst_feature)\n",
    "                if verbose:\n",
    "                    print(f'Drop {worst_feature:30} with p-value {worst_pval:.6f}')\n",
    "\n",
    "        # Se não houve mudança (nenhuma inclusão ou exclusão), o processo termina\n",
    "        if not changed:\n",
    "            break\n",
    "    \n",
    "    return included\n",
    "\n",
    "# Dados de exemplo (você deve substituir por seus dados reais)\n",
    "# Certifique-se de que X e y estão prontos para a modelagem\n",
    "# X é o DataFrame de variáveis independentes\n",
    "# y é a variável dependente\n",
    "variaveis = stepwise_selection(X_train, y_train)\n",
    "\n",
    "print('resulting features:')\n",
    "print(variaveis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 OLS Regression Results                                \n",
      "=======================================================================================\n",
      "Dep. Variable:                  renda   R-squared (uncentered):                   0.474\n",
      "Model:                            OLS   Adj. R-squared (uncentered):              0.474\n",
      "Method:                 Least Squares   F-statistic:                              1398.\n",
      "Date:                Fri, 03 Jan 2025   Prob (F-statistic):                        0.00\n",
      "Time:                        22:20:44   Log-Likelihood:                         -97098.\n",
      "No. Observations:                9320   AIC:                                  1.942e+05\n",
      "Df Residuals:                    9314   BIC:                                  1.943e+05\n",
      "Df Model:                           6                                                  \n",
      "Covariance Type:            nonrobust                                                  \n",
      "==============================================================================================\n",
      "                                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------------------\n",
      "sexo_M                      5819.7116    168.768     34.484      0.000    5488.890    6150.533\n",
      "tempo_emprego                572.0125     13.245     43.186      0.000     546.049     597.976\n",
      "tipo_renda_Empresário        630.9407    186.769      3.378      0.001     264.833     997.048\n",
      "idade                        -15.9507      4.830     -3.303      0.001     -25.418      -6.483\n",
      "educacao_Superior completo   299.5552    169.575      1.767      0.077     -32.849     631.959\n",
      "posse_de_imovel              126.8809    174.312      0.728      0.467    -214.809     468.571\n",
      "==============================================================================\n",
      "Omnibus:                    13672.407   Durbin-Watson:                   1.995\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):          8319620.408\n",
      "Skew:                           8.717   Prob(JB):                         0.00\n",
      "Kurtosis:                     148.327   Cond. No.                         96.9\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n",
      "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "relevant_features = ['sexo_M', 'tempo_emprego', 'tipo_renda_Empresário', \n",
    "                     'idade', 'educacao_Superior completo', 'posse_de_imovel']\n",
    "\n",
    "# Criar subconjuntos dos dados\n",
    "X_relevant = X_train[relevant_features]\n",
    "y_train_log = np.log(y_train)\n",
    "\n",
    "# Ajustar o modelo OLS\n",
    "#ols_model = sm.OLS(y_train, X_relevant).fit()\n",
    "ols_model_log = sm.OLS(y_train_log, X_relevant).fit()\n",
    "\n",
    "# Resumo do modelo\n",
    "print(ols_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared on Test Set: 0.2926\n",
      "\n",
      "First 10 predicted vs real values:\n",
      "          Predicted     Real\n",
      "14812   6231.557792  1705.55\n",
      "11591   6379.142935  1748.99\n",
      "13436    330.539309  1733.67\n",
      "14948   7255.872333  2378.25\n",
      "14509   3899.387804  1957.87\n",
      "2955    2562.385207  1218.39\n",
      "7148    -663.873294  2085.39\n",
      "1337   10454.594734  8849.50\n",
      "14688    634.940719  4676.05\n",
      "10501    766.702786  1915.48\n"
     ]
    }
   ],
   "source": [
    "X_relevant_test = X_test[relevant_features]\n",
    "y_pred = ols_model.predict(X_relevant_test)\n",
    "\n",
    "\n",
    "# R² (coeficiente de determinação)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'R-squared on Test Set: {r2:.4f}')\n",
    "\n",
    "# Opcional: Exibir as primeiras previsões comparadas com os valores reais\n",
    "print(\"\\nFirst 10 predicted vs real values:\")\n",
    "print(pd.DataFrame({'Predicted': y_pred[:10], 'Real': y_test[:10]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusão:\n",
    "- Modelo OLS foi mais interpretável, pois usou apenas as variáveis com p-values significativos, mas possivelmente sofreu de overfitting ao se ajustar excessivamente aos dados de treinamento. Esse modelo pode ter um desempenho satisfatório em termos de R², mas provavelmente teve dificuldades em generalizar para o conjunto de teste.\n",
    "\n",
    "- Ridge e Lasso apresentaram um R² mais baixo, mas sua capacidade de regularização ajudou a controlar o overfitting e a melhorar a generalização. Em situações com muitas variáveis e potencial multicolinearidade, esses modelos tendem a ser mais robustos e a ter um desempenho mais consistente em dados não vistos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² da árvore de regressão: 0.2171\n"
     ]
    }
   ],
   "source": [
    "# Ajustar o modelo de árvore de regressão\n",
    "tree_model = DecisionTreeRegressor(random_state=42)\n",
    "tree_model.fit(X_relevant, y_train)\n",
    "\n",
    "# Fazer previsões no conjunto de teste\n",
    "y_pred_tree = tree_model.predict(X_relevant_test)\n",
    "\n",
    "# Calcular o R²\n",
    "r2_tree = r2_score(y_test, y_pred_tree)\n",
    "\n",
    "# Exibir o R² do modelo de árvore de regressão\n",
    "print(f'R² da árvore de regressão: {r2_tree:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
